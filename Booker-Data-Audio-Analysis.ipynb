{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Booker Data Audio Analysis\n",
    "\n",
    "After reading the Booker Paper (i.e, \"Narratives in the Immediate Aftermath of Traumatic Injury: Markers of Ongoing Depressive and Posttraumatic Stress Disorder Symptoms\"), we noticed that it just focusses on the words spoken, and not the **way these words were spoken**.\n",
    "\n",
    "In addiiton, reading other papers by prominent researchers such as Dr. David C Atkins, I also am curious regarding **measuring empathy of the interviewer** in the ED scenario. Empathy expressed by the therapist is also an indicator to recovery speed of the patient.\n",
    "\n",
    "So here's what I propose. Let's use the Booker Paper Data, and apart from extracting the features they have mentioned in their paper, we can augment and create a more accurate model by including audio features from the patient audio, and empathy measurement on the interviewer speech. For Empathy calculation, we can follow something along the lines of what Dr. Atkins has demonstrated in his paper:  A Computational Approach to Understanding Empathy Expressed inText-Based Mental Health Support (https://arxiv.org/pdf/2009.08441.pdf)\n",
    "\n",
    "So our model would consist of:\n",
    "1. Features mentioned in the Booker Paper.\n",
    "2. Audio Features extracted from patient audio.\n",
    "3. Empathy calculation from therapist audio.\n",
    "\n",
    "All these together could be a novel addition to the already existing work, and give our paper a more HCI-oriented direction.\n",
    "\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "I have maintained the following structure throughout the notebook:\n",
    "1. A brief description of that particular feature is presented.\n",
    "2. Helper Functions are declared that can be eventually called for a particular audio file.\n",
    "3. Helper Functions are then called for the ED audio file, and the subsequent follow-up sessions. The extracted values are printed, graphs are plotted to gelp us compare the result.\n",
    "4. Observation, if any, are mentioned.\n",
    "\n",
    "\n",
    "## Features Extracted\n",
    "\n",
    "This notebook deals with \"audio feature extraction\" part of our research. The list of features Extracted are:  \n",
    "1. Number of Onsets\n",
    "2. Pitch Estimation\n",
    "1. MFCC\n",
    "2. Zero Crossings\n",
    "3. Spectral Centroid\n",
    "4. Spectrogram\n",
    "5. Chroma FFT\n",
    "6. Energy\n",
    "7. RMS Energy\n",
    "8. Spectral Rolloff\n",
    "9. Phonation Rate\n",
    "10. Speech Productivity\n",
    "11. Speech Rate\n",
    "12. Articulation Rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting off: Importing required modules and loading the three audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librosa sample rate: 22050\n"
     ]
    }
   ],
   "source": [
    "import librosa \n",
    "import librosa.display\n",
    "from scipy.io import wavfile as wav\n",
    "import speech_recognition as sr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis\n",
    "import sklearn\n",
    "from pydub import AudioSegment \n",
    "from pydub.silence import split_on_silence \n",
    "import ffmpeg\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Audio File Recorded at ED (hereby referred to as ED Audio)\n",
    "filename1 = 'patient_audio/session2/low_sud_audio.wav'\n",
    "duration1 = librosa.get_duration(filename=filename1)\n",
    "\n",
    "# Audio File Recorded at first Follow-Up Session (hereby referred to as FLW-1 Audio)\n",
    "filename2 = 'patient_audio/session2/medium_sud_audio.wav'\n",
    "duration2 = librosa.get_duration(filename=filename2)\n",
    "\n",
    "# Audio File Recorded at second Follow-Up Session (hereby referred to as FLW-2 Audio)\n",
    "filename3 = 'patient_audio/session2/high_sud_audio.wav'\n",
    "duration3 = librosa.get_duration(filename=filename3)\n",
    "\n",
    "'''\n",
    "In order to compare the three audio files properly, we should be analyzing equal sizes of audio snippets.\n",
    "Please ensure that all audio snippets are 50 seconds long each, covering the audio of the most interest. \n",
    "Otherwise the code below is gonna snip it anyways.\n",
    "Thus, snipping duration of all three audio files to the minimum of the three files.\n",
    "'''\n",
    "audio_duration = min(duration1, duration2, duration3)\n",
    "\n",
    "flw_2_audio, sample_rate = librosa.load(filename1, duration=audio_duration) \n",
    "flw_1_audio, sample_rate = librosa.load(filename2, duration=audio_duration) \n",
    "ed_audio, sample_rate = librosa.load(filename3, duration=audio_duration) \n",
    "\n",
    "print('Librosa sample rate:', sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Onset Detection\n",
    "\n",
    "Onset refers to the beginning of a musical note or other sound. It is related to (but different from) the concept of a transient: all musical notes have an onset, but do not necessarily include an initial transient. We are locating note onset events by picking peaks in an onset strength envelope here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "'''\n",
    "Creating Helper Function that checks number of onsets. \n",
    "    Param1: The Audio\n",
    "'''\n",
    "hop_length = 512\n",
    "def number_of_onsets(audio):\n",
    "    onset_frames = librosa.onset.onset_detect(audio, sr=sample_rate, hop_length=hop_length, backtrack=True)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sample_rate, hop_length=hop_length)\n",
    "    return(len(onset_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Onsets for ED Audio: 319\n",
      "Number of Onsets for FLW-1 Audio: 377\n",
      "Number of Onsets for FLW-2 Audio: 387\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Onsets for ED Audio: \"+str(number_of_onsets(ed_audio)))\n",
    "print(\"Number of Onsets for FLW-1 Audio: \"+str(number_of_onsets(flw_1_audio)))\n",
    "print(\"Number of Onsets for FLW-2 Audio: \"+str(number_of_onsets(flw_2_audio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation for Onset Detections\n",
    "\n",
    "Higher the SUD, lesser onsets were detected. Not sure about the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Pitch Estimation\n",
    "\n",
    "The lowest frequency of any vibrating object is called the fundamental frequency. The fundamental frequency provides the sound with its strongest audible pitch reference - it is the predominant frequency in any complex waveform.\n",
    "\n",
    "A sine wave is the simplest of all waveforms and contains only a single fundamental frequency and no harmonics, overtones or partials.\n",
    "\n",
    "Virtually all musical sounds have waves that are infinitely more complex than a sine wave. It is the addition of harmonics and overtones to a wave that makes it possible to distinguish between different sounds and instruments; the timbre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "'''\n",
    "Creating Helper Function that estimates pitch. \n",
    "    Param1: The Audio\n",
    "'''\n",
    "def find_pitch(audio):\n",
    "    \n",
    "    r = librosa.autocorrelate(audio, max_size=10000)\n",
    "    midi_hi = 120.0\n",
    "    midi_lo = 12.0\n",
    "    f_hi = librosa.midi_to_hz(midi_hi)\n",
    "    f_lo = librosa.midi_to_hz(midi_lo)\n",
    "    t_lo = sample_rate/f_hi\n",
    "    t_hi = sample_rate/f_lo\n",
    "    \n",
    "    r[:int(t_lo)] = 0\n",
    "    r[int(t_hi):] = 0\n",
    "    t_max = r.argmax()\n",
    "    f0 = (sample_rate)/t_max\n",
    "    return f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch for ED Audio: 11025.0 Hz\n",
      "Pitch for FLW-1 Audio: 11025.0 Hz\n",
      "Pitch for FLW-2 Audio: 11025.0 Hz\n"
     ]
    }
   ],
   "source": [
    "print(\"Pitch for ED Audio: \"+str(find_pitch(ed_audio))+\" Hz\")\n",
    "print(\"Pitch for FLW-1 Audio: \"+str(find_pitch(flw_1_audio))+\" Hz\")\n",
    "print(\"Pitch for FLW-2 Audio: \"+str(find_pitch(flw_2_audio))+\" Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation for Pitch Estimation\n",
    "\n",
    "Pitch Estimates coming out to be exactly same for all audio files. Will need to test on other audio files to understand it's relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Spectral Bandwidth\n",
    "\n",
    "Bandwidth is the difference between the upper and lower frequencies in a continuous band of frequencies. It is typically measured in hertz.\n",
    "\n",
    "The below function computes bandwidth across many time frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "'''\n",
    "Creating Helper Function that calculates variance of order-p spectral bandwidth. \n",
    "    Param1: The Audio\n",
    "    Param2: p\n",
    "'''\n",
    "def bandwidth_mean(audio, p):\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(audio+0.01, sr=sample_rate, p=p)[0]\n",
    "    return (np.mean(spectral_bandwidth, dtype = np.float32))\n",
    "\n",
    "def bandwidth_variance(audio, p):\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(audio+0.01, sr=sample_rate, p=p)[0]\n",
    "    return (np.var(spectral_bandwidth, dtype = np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-order Spectral Bandwidth mean for ED Audio: 2286.667\n",
      "2-order Spectral Bandwidth variance for ED Audio: 196597.48\n",
      "\n",
      "\n",
      "2-order Spectral Bandwidth mean for FLW-1 Audio: 2191.14\n",
      "2-order Spectral Bandwidth variance for FLW-1 Audio: 205327.84\n",
      "\n",
      "\n",
      "2-order Spectral Bandwidth mean for FLW-2 Audio: 2201.5193\n",
      "2-order Spectral Bandwidth variance for FLW-2 Audio: 240072.23\n"
     ]
    }
   ],
   "source": [
    "# Checking second order spectral bandwidths\n",
    "print(\"2-order Spectral Bandwidth mean for ED Audio: \"+str(bandwidth_mean(ed_audio, 2)))\n",
    "print(\"2-order Spectral Bandwidth variance for ED Audio: \"+str(bandwidth_variance(ed_audio, 2))+\"\\n\\n\")\n",
    "print(\"2-order Spectral Bandwidth mean for FLW-1 Audio: \"+str(bandwidth_mean(flw_1_audio, 2)))\n",
    "print(\"2-order Spectral Bandwidth variance for FLW-1 Audio: \"+str(bandwidth_variance(flw_1_audio, 2))+\"\\n\\n\")\n",
    "print(\"2-order Spectral Bandwidth mean for FLW-2 Audio: \"+str(bandwidth_mean(flw_2_audio, 2)))\n",
    "print(\"2-order Spectral Bandwidth variance for FLW-2 Audio: \"+str(bandwidth_variance(flw_2_audio, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-order Spectral Bandwidth mean for ED Audio: 2884.796\n",
      "3-order Spectral Bandwidth variance for ED Audio: 124205.945\n",
      "\n",
      "\n",
      "3-order Spectral Bandwidth mean for FLW-1 Audio: 2797.852\n",
      "3-order Spectral Bandwidth variance for FLW-1 Audio: 140705.19\n",
      "\n",
      "\n",
      "3-order Spectral Bandwidth mean for FLW-2 Audio: 2784.9329\n",
      "3-order Spectral Bandwidth variance for FLW-2 Audio: 172804.69\n"
     ]
    }
   ],
   "source": [
    "# Checking third order spectral bandwidths\n",
    "print(\"3-order Spectral Bandwidth mean for ED Audio: \"+str(bandwidth_mean(ed_audio, 3)))\n",
    "print(\"3-order Spectral Bandwidth variance for ED Audio: \"+str(bandwidth_variance(ed_audio, 3))+\"\\n\\n\")\n",
    "print(\"3-order Spectral Bandwidth mean for FLW-1 Audio: \"+str(bandwidth_mean(flw_1_audio, 3)))\n",
    "print(\"3-order Spectral Bandwidth variance for FLW-1 Audio: \"+str(bandwidth_variance(flw_1_audio, 3))+\"\\n\\n\")\n",
    "print(\"3-order Spectral Bandwidth mean for FLW-2 Audio: \"+str(bandwidth_mean(flw_2_audio, 3)))\n",
    "print(\"3-order Spectral Bandwidth variance for FLW-2 Audio: \"+str(bandwidth_variance(flw_2_audio, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation for Spectral Bandwidth\n",
    "\n",
    "Higher the SUD, lower the bandwidth variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Mel-Frequency Cepstral Coefficients (MFCC)\n",
    "\n",
    "The MFCC is an immensely elaborate concept in itself, and thus might require extra reading to fully understand.\n",
    "However, to put it in a nutshell, it can be best described as the **rate of change in spectral bands**, i.e, it's the rate of change in various frequencies that are present in the audio. For a detailed and mathematical understanding of MFCC, you check out https://medium.com/prathena/the-dummys-guide-to-mfcc-aceab2450fd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "'''\n",
    "Creating Helper Function that visualizes the MFCC Values. \n",
    "    Param1: The MFCC Array\n",
    "    Param2: The coefficient you wish to visualize\n",
    "'''\n",
    "\n",
    "'''\n",
    "Creating Helper Function that calculates the Average of MFCC Values. \n",
    "    Param1: The MFCC Array\n",
    "'''\n",
    "def mfcc_mean(mfcc_value, coefficient_number):\n",
    "    return np.mean(mfcc_value[coefficient_number])\n",
    "\n",
    "\n",
    "'''\n",
    "Creating Helper Function that calculates the Variance of MFCC Values. \n",
    "    Param1: The MFCC Array\n",
    "    Param2: The coefficient\n",
    "'''\n",
    "def mfcc_variance(mfcc_value, coefficient_number):\n",
    "    return np.var(mfcc_value[coefficient_number], dtype = np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting MFCC Values\n",
    "mfccs_ed_audio = librosa.feature.mfcc(y=ed_audio, sr=sample_rate, n_mfcc=20)\n",
    "mfccs_flw_1_audio = librosa.feature.mfcc(y=flw_1_audio, sr=sample_rate, n_mfcc=20)\n",
    "mfccs_flw_2_audio = librosa.feature.mfcc(y=flw_2_audio, sr=sample_rate, n_mfcc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ED Audio: \n",
      "Mean MFCC Values for 2nd Coefficent: -4.792346\n",
      "MFCC Values Variance for 2nd Coefficent: 405.71127\n",
      "\n",
      "For FLW-1 Audio: \n",
      "Mean MFCC Values for 2nd Coefficent: -3.5600028\n",
      "MFCC Values Variance for 2nd Coefficent: 439.09656\n",
      "\n",
      "For FLW-2 Audio: \n",
      "Mean MFCC Values for 2nd Coefficent: -6.3759904\n",
      "MFCC Values Variance for 2nd Coefficent: 552.8272\n"
     ]
    }
   ],
   "source": [
    "# For ED Audio\n",
    "print(\"For ED Audio: \")\n",
    "print(\"Mean MFCC Values for 2nd Coefficent: \"+str(mfcc_mean(mfccs_ed_audio, 2)))\n",
    "print(\"MFCC Values Variance for 2nd Coefficent: \"+str(mfcc_variance(mfccs_ed_audio, 2))+\"\\n\")\n",
    "\n",
    "# For FLW-1 Audio\n",
    "print(\"For FLW-1 Audio: \")\n",
    "print(\"Mean MFCC Values for 2nd Coefficent: \"+str(mfcc_mean(mfccs_flw_1_audio, 2)))\n",
    "print(\"MFCC Values Variance for 2nd Coefficent: \"+str(mfcc_variance(mfccs_flw_1_audio, 2))+\"\\n\")\n",
    "\n",
    "# For FLW-2 Audio\n",
    "print(\"For FLW-2 Audio: \")\n",
    "print(\"Mean MFCC Values for 2nd Coefficent: \"+str(mfcc_mean(mfccs_flw_2_audio, 2)))\n",
    "print(\"MFCC Values Variance for 2nd Coefficent: \"+str(mfcc_variance(mfccs_flw_2_audio, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do similar analysis for all 20 MFCC coefficients\n",
    "\n",
    "In the above example, we have passed coeeficient as 2. We can do the same analysis for coefficents 0-19. For the second coefficient, we see that variance increases as SUD decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Zero Crossings (ZC)\n",
    "\n",
    "The zero crossing rate is the rate of sign-changes along a signal, i.e., the rate at which the signal changes from positive to negative or back. This feature has been used heavily in both speech recognition and music information retrieval. It usually has higher values for highly percussive sounds like those in metal and rock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "'''\n",
    "Creating Helper Function that calculates the Mean of ZC Values. \n",
    "    Param1: The ZC Array\n",
    "'''\n",
    "def zc_mean(zero_crossings):\n",
    "    return np.mean(zero_crossings, dtype = np.float32)\n",
    "'''\n",
    "Creating Helper Function that calculates the Variance of ZC Values. \n",
    "    Param1: The ZC Array\n",
    "'''\n",
    "def zc_variance(zero_crossings):\n",
    "    return np.var(zero_crossings, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Crossings for ED Audio: 289482\n",
      "Mean Zero Crossings for ED Audio: 0.109551355\n",
      "Variance of Zero Crossings for ED Audio: 0.09754987\n",
      "\n",
      "Zero Crossings for FLW-1 Audio: 243075\n",
      "Mean Zero Crossings for FLW-1 Audio: 0.09198912\n",
      "Variance of Zero Crossings for FLW-1 Audio: 0.08352713\n",
      "\n",
      "Zero Crossings for FLW-2 Audio: 271281\n",
      "Mean Zero Crossings for FLW-2 Audio: 0.10266338\n",
      "Variance of Zero Crossings for FLW-2 Audio: 0.092123665\n"
     ]
    }
   ],
   "source": [
    "# For ED Audio\n",
    "zero_crossings_ed_audio = librosa.zero_crossings(ed_audio, pad=False)\n",
    "print(\"Zero Crossings for ED Audio: \"+str(sum(zero_crossings_ed_audio)))\n",
    "print(\"Mean Zero Crossings for ED Audio: \"+str(zc_mean(zero_crossings_ed_audio)))\n",
    "print(\"Variance of Zero Crossings for ED Audio: \"+str(zc_variance(zero_crossings_ed_audio))+\"\\n\")\n",
    "\n",
    "# For FLW-1 Audio\n",
    "zero_crossings_flw_1_audio = librosa.zero_crossings(flw_1_audio, pad=False)\n",
    "print(\"Zero Crossings for FLW-1 Audio: \"+str(sum(zero_crossings_flw_1_audio)))\n",
    "print(\"Mean Zero Crossings for FLW-1 Audio: \"+str(zc_mean(zero_crossings_flw_1_audio)))\n",
    "print(\"Variance of Zero Crossings for FLW-1 Audio: \"+str(zc_variance(zero_crossings_flw_1_audio))+\"\\n\")\n",
    "\n",
    "# For FLW-2 Audio\n",
    "zero_crossings_flw_2_audio = librosa.zero_crossings(flw_2_audio, pad=False)\n",
    "print(\"Zero Crossings for FLW-2 Audio: \"+str(sum(zero_crossings_flw_2_audio)))\n",
    "print(\"Mean Zero Crossings for FLW-2 Audio: \"+str(zc_mean(zero_crossings_flw_2_audio)))\n",
    "print(\"Variance of Zero Crossings for FLW-2 Audio: \"+str(zc_variance(zero_crossings_flw_2_audio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for Zero Crossings (ZC):\n",
    "\n",
    "Interesting stuff! ZC is a measure of noisiness of a signal. It's known to have HIGHER variance for music-like audio segments, and LOWER variance for speech-like audio segments. \n",
    "\n",
    "From our observation above, we can see that higher the SUD value, HIGHER is the variance, i.e, indicating a MORE NOISY audio as SUDs increase. I believe this is happening due to the patient breaking down and crying at higher SUDs, thereby leading to more noisiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Spectral Centroid (SC)\n",
    "\n",
    "The spectral centroid indicates at which frequency the energy of a spectrum is centered upon or in other words It indicates where the **center of mass** for a sound is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "'''\n",
    "Creating Helper Function that normalizes. \n",
    "'''\n",
    "\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "\n",
    "'''\n",
    "Creating Helper Function that calculates mean of spectral centroids. \n",
    "'''\n",
    "def centroid_mean(audio):\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(audio, sr=sample_rate)[0]\n",
    "    return np.mean(spectral_centroids, dtype = np.float32)\n",
    "\n",
    "'''\n",
    "Creating Helper Function that calculates variance of spectral centroids. \n",
    "'''\n",
    "def centroid_variance(audio):\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(audio, sr=sample_rate)[0]\n",
    "    return np.var(spectral_centroids, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Spectral Centroids for ED audio: 2249.0615\n",
      "Variance of Spectral Centroids for ED audio: 728736.9\n",
      "\n",
      "Mean of Spectral Centroids for FLW-1 audio: 2083.7686\n",
      "Variance of Spectral Centroids for FLW-2 audio: 660445.3\n",
      "\n",
      "Mean of Spectral Centroids for FLW-2 audio: 2168.2783\n",
      "Variance of Spectral Centroids for FLW-2 audio: 830903.0\n"
     ]
    }
   ],
   "source": [
    "# For ED Audio\n",
    "print(\"Mean of Spectral Centroids for ED audio: \"+str(centroid_mean(ed_audio)))\n",
    "print(\"Variance of Spectral Centroids for ED audio: \"+str(centroid_variance(ed_audio))+\"\\n\")\n",
    "\n",
    "# For FLW-1 Audio\n",
    "print(\"Mean of Spectral Centroids for FLW-1 audio: \"+str(centroid_mean(flw_1_audio)))\n",
    "print(\"Variance of Spectral Centroids for FLW-1 audio: \"+str(centroid_variance(flw_1_audio))+\"\\n\")\n",
    "\n",
    "# For FLW-2 Audio\n",
    "print(\"Mean of Spectral Centroids for FLW-2 audio: \"+str(centroid_mean(flw_2_audio)))\n",
    "print(\"Variance of Spectral Centroids for FLW-2 audio: \"+str(centroid_variance(flw_2_audio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for Spectral Centroids (SC):\n",
    "\n",
    "Spectral Centroid can be best thought of as the dominant frequency at a certain point of time. Now according to the Marmar paper, people with more monotonous speech (i.e, less variation in frequencies) are more prone to PTSD, but the results we got above are in direct contrast.\n",
    "\n",
    "We see that, higher the SUD value, MORE is the variance in spectral centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Chroma 12 Pitch Scale \n",
    "\n",
    "In music, the term chroma feature or chromagram closely relates to the twelve different pitch classes.\n",
    "\n",
    "The underlying observation is that humans perceive two musical pitches as similar in color if they differ by an octave. Based on this observation, a pitch can be separated into two components, which are referred to as tone height and chroma. Assuming the equal-tempered scale, one considers twelve chroma values represented by the set\n",
    "\n",
    "{C, C♯, D, D♯, E , F, F♯, G, G♯, A, A♯, B}\n",
    "\n",
    "The Marmar paper has extensively used this particular feature, so I too extracted it to check how Chroma FFT values differ accross audio files of different SUD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "'''\n",
    "Creating Helper Function that calculates mean of the chromagram. \n",
    "    Param1: Path to audio file\n",
    "    Param2: Coefficient Number\n",
    "'''\n",
    "\n",
    "def chroma_mean(audio, coefficient_number):\n",
    "    chromagram = librosa.feature.chroma_stft(audio, sr=sample_rate, hop_length=512)\n",
    "    return np.mean(chromagram[coefficient_number], dtype = np.float32)\n",
    "\n",
    "'''\n",
    "Creating Helper Function that calculates variance of the chromagram. \n",
    "    Param1: Path to audio file\n",
    "    Param2: Coefficient Number\n",
    "'''\n",
    "\n",
    "def chroma_variance(audio, coefficient_number):\n",
    "    chromagram = librosa.feature.chroma_stft(audio, sr=sample_rate, hop_length=512)\n",
    "    return np.var(chromagram[coefficient_number], dtype = np.float32)\n",
    "\n",
    "\n",
    "'''\n",
    "Creating Helper Function that calculates kurtosis of the chromagram. \n",
    "    Param1: Path to audio file\n",
    "    Param2: Coefficient Number\n",
    "'''\n",
    "\n",
    "def chroma_kurtosis(audio, coefficient_number):\n",
    "    chromagram = librosa.feature.chroma_stft(audio, sr=sample_rate, hop_length=512)\n",
    "    return kurtosis(chromagram[coefficient_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of first Chroma FFT for ED Audio: 0.50464016\n",
      "Variance of first Chroma FFT for FLW-1 Audio: 0.09402144\n",
      "\n",
      "Mean of first Chroma FFT for FLW-1 Audio: 0.49902692\n",
      "Variance of first Chroma FFT for FLW-1 Audio: 0.08909191\n",
      "\n",
      "Mean of first Chroma FFT for FLW-2 Audio: 0.48722297\n",
      "Variance of first Chroma FFT for FLW-2 Audio: 0.09145144\n"
     ]
    }
   ],
   "source": [
    "# For ED Audio\n",
    "print(\"Mean of first Chroma FFT for ED Audio: \"+str(chroma_mean(ed_audio, 1)))\n",
    "print(\"Variance of first Chroma FFT for FLW-1 Audio: \"+str(chroma_variance(ed_audio, 1))+\"\\n\")\n",
    "\n",
    "# For FLW-1 Audio\n",
    "print(\"Mean of first Chroma FFT for FLW-1 Audio: \"+str(chroma_mean(flw_1_audio, 1)))\n",
    "print(\"Variance of first Chroma FFT for FLW-1 Audio: \"+str(chroma_variance(flw_1_audio, 1))+\"\\n\")\n",
    "\n",
    "# For FLW-2 Audio\n",
    "print(\"Mean of first Chroma FFT for FLW-2 Audio: \"+str(chroma_mean(flw_2_audio, 1)))\n",
    "print(\"Variance of first Chroma FFT for FLW-2 Audio: \"+str(chroma_variance(flw_2_audio, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do similar analysis for all 12 Chroma coefficients\n",
    "\n",
    "In the above example, we have passed coeeficient as 1. We can do the same analysis for coefficents 0-11. For the first coefficient, we see that mean decreases as SUD decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: RMS Energy\n",
    "\n",
    "The energy of a signal corresponds to the total magntiude of the signal. For audio signals, that roughly corresponds to how loud the signal is. The RMS energy is just the root mean square of that energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "'''\n",
    "Creating Helper Function that calculates mean in RMSE values \n",
    "    Param1: Audio\n",
    "'''\n",
    "def rmse_mean(audio):\n",
    "    hop_length = 256\n",
    "    frame_length = 512\n",
    "\n",
    "    rmse = librosa.feature.rms(audio, frame_length=frame_length, hop_length=hop_length, center=True)\n",
    "    rmse = rmse[0]\n",
    "\n",
    "    return np.mean(rmse, dtype = np.float32)   \n",
    "    \n",
    "'''\n",
    "Creating Helper Function that calculates variance in RMSE values \n",
    "    Param1: Audio\n",
    "'''\n",
    "def rmse_variance(audio):\n",
    "    hop_length = 256\n",
    "    frame_length = 512\n",
    "\n",
    "    rmse = librosa.feature.rms(audio, frame_length=frame_length, hop_length=hop_length, center=True)\n",
    "    rmse = rmse[0]\n",
    "\n",
    "    return np.var(rmse, dtype = np.float32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE for ED audio: 0.0101291025\n",
      "RMS Variance for ED audio: 9.459419e-05\n",
      "\n",
      "Mean RMSE for FLW-1 audio: 0.008702356\n",
      "RMS Variance for FLW-1 audio: 4.6746158e-05\n",
      "\n",
      "Mean RMSE for FLW-2 audio: 0.0105046835\n",
      "RMS Variance for FLW-2 audio: 9.3368464e-05\n"
     ]
    }
   ],
   "source": [
    "# For ED Audio\n",
    "print(\"Mean RMSE for ED audio: \"+str(rmse_mean(ed_audio)))\n",
    "print(\"RMS Variance for ED audio: \"+str(rmse_variance(ed_audio))+\"\\n\")\n",
    "\n",
    "# For FLW-1 Audio\n",
    "print(\"Mean RMSE for FLW-1 audio: \"+str(rmse_mean(flw_1_audio)))\n",
    "print(\"RMS Variance for FLW-1 audio: \"+str(rmse_variance(flw_1_audio))+\"\\n\")\n",
    "\n",
    "# For FLW-2 Audio\n",
    "print(\"Mean RMSE for FLW-2 audio: \"+str(rmse_mean(flw_2_audio)))\n",
    "print(\"RMS Variance for FLW-2 audio: \"+str(rmse_variance(flw_2_audio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for RMS Energy:\n",
    "\n",
    "It's difficult to point out any observations as such, since we see no general trend here. Maybe analyzing the audio of another session might help to discover some trend. \n",
    "\n",
    "Again, according to my intuition, RMS Energy's variance should DECREASE with higher SUDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Spectral Rolloff\n",
    "\n",
    "Spectral rolloff is the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lies.\n",
    "\n",
    "Neither discussed in Marmar or Wiegersma papers, but I was curious to see how they vary for different SUD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "'''\n",
    "Creating Helper Function that calculates mean of the calculated rolloffs. \n",
    "    Param1: Audio\n",
    "'''\n",
    "\n",
    "def rolloff_mean(audio):\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(audio+0.01, sr=sample_rate)[0]\n",
    "    return np.mean(spectral_rolloff, dtype = np.float32)\n",
    "\n",
    "'''\n",
    "Creating Helper Function that calculates variance of the calculated rolloffs. \n",
    "    Param1: Audio\n",
    "'''\n",
    "\n",
    "def rolloff_variance(audio):\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(audio+0.01, sr=sample_rate)[0]\n",
    "    return np.var(spectral_rolloff, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rolloff for ED audio: 4112.696\n",
      "Rolloff Variance for ED audio: 2892565.5\n",
      "\n",
      "Mean Rolloff for FLW-1 audio: 3774.2007\n",
      "Rolloff Variance for FLW-1 audio: 2456047.5\n",
      "\n",
      "Mean Rolloff for FLW-2 audio: 3939.07\n",
      "Rolloff Variance for FLW-2 audio: 2738746.5\n"
     ]
    }
   ],
   "source": [
    "# For ED Audio\n",
    "print(\"Mean Rolloff for ED audio: \"+str(rolloff_mean(ed_audio)))\n",
    "print(\"Rolloff Variance for ED audio: \"+str(rolloff_variance(ed_audio))+\"\\n\")\n",
    "\n",
    "# For FLW-1 Audio\n",
    "print(\"Mean Rolloff for FLW-1 audio: \"+str(rolloff_mean(flw_1_audio)))\n",
    "print(\"Rolloff Variance for FLW-1 audio: \"+str(rolloff_variance(flw_1_audio))+\"\\n\")\n",
    "\n",
    "# For FLW-2 Audio\n",
    "print(\"Mean Rolloff for FLW-2 audio: \"+str(rolloff_mean(flw_2_audio)))\n",
    "print(\"Rolloff Variance for FLW-2 audio: \"+str(rolloff_variance(flw_2_audio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for Spectral Rolloff (SR):\n",
    "\n",
    "It's difficult to point out any observations as such, since we see no general trend here. Maybe analyzing the audio of another session might help to discover some trend. \n",
    "\n",
    "Again, according to my intuition, Rolloff variance should DECREASE with higher SUDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction: Prosodic Features\n",
    "\n",
    "Prosody refers to a collection of acoustic features that concern intonation-related (pitch), loudness-related\n",
    "(intensity),and tempo-related(e.g. durational aspects, speaking rate) features. This can closely contribute to meaning and may reveal information normally not captured by textual features, such as emotional state or attitude.\n",
    "\n",
    "The Wiegersma paper has used this particular feature for a certain part of their experiment, so I too extracted it to check how prosodic features values differ accross audio files of different SUD values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "'''\n",
    "Creating Helper Function that calculates Prosodic Features for given audio file. \n",
    "    Param1: Path to audio file\n",
    "'''\n",
    "def find_prosodic_features(path):\n",
    "    prosodic_features = {\n",
    "        \"phonation_rate\" : 0,\n",
    "        \"speech_productivity\" : 0,\n",
    "        \"speech_rate\" : 0,\n",
    "        \"articulation_rate\" : 0\n",
    "    }\n",
    "    \n",
    "    audio_for_prosody, sample_rate = librosa.load(path, duration=audio_duration) \n",
    "    \n",
    "    # Finding voiced intervals by removing silent parts of the audio\n",
    "    voiced_intervals = librosa.effects.split(y=audio_for_prosody, top_db=20)\n",
    "    total_voiced_duration = 0\n",
    "    for interval in voiced_intervals:\n",
    "        total_voiced_duration = total_voiced_duration + ((interval[1]-interval[0])/sample_rate)\n",
    "    \n",
    "    # To account for overflows\n",
    "    if total_voiced_duration > audio_duration:\n",
    "        total_voiced_duration = audio_duration\n",
    "        \n",
    "    total_silenced_duration = audio_duration-total_voiced_duration\n",
    "\n",
    "    prosodic_features[\"phonation_rate\"] = total_voiced_duration/audio_duration\n",
    "    prosodic_features[\"speech_productivity\"] = (total_silenced_duration)/total_voiced_duration\n",
    "\n",
    "    # Reading Audio file as source\n",
    "    # listening the audio file and store in audio_text variable\n",
    "    r = sr.Recognizer()\n",
    "    demo = sr.AudioFile(path)\n",
    "    with demo as source:\n",
    "\n",
    "        audio = r.record(source, duration=50)\n",
    "\n",
    "        # recognize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
    "        try:\n",
    "\n",
    "            # using google speech recognition\n",
    "            text = r.recognize_google(audio)\n",
    "            num_words_spoken = len(text.split())\n",
    "            prosodic_features[\"speech_rate\"] = num_words_spoken/audio_duration\n",
    "            prosodic_features[\"articulation_rate\"] = num_words_spoken/total_voiced_duration\n",
    "\n",
    "        except:\n",
    "             print('Error in Calculating speech rate and articulation rate')\n",
    "    \n",
    "    return prosodic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prosodic Features for ED Audio: \n",
      "{'phonation_rate': 0.39992249564037957, 'speech_productivity': 1.5004844961240322, 'speech_rate': 0.03337834237550862, 'articulation_rate': 0.08346202761627912}\n",
      "\n",
      "Prosodic Features for FLW-1 Audio: \n",
      "{'phonation_rate': 0.6039527223406315, 'speech_productivity': 0.6557587423804947, 'speech_rate': 0.05006751356326294, 'articulation_rate': 0.0828997232916266}\n",
      "\n",
      "Prosodic Features for FLW-2 Audio: \n",
      "{'phonation_rate': 0.4826583995349738, 'speech_productivity': 1.0718586912886392, 'speech_rate': 0.08344585593877156, 'articulation_rate': 0.17288802187876356}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For ED Audio\n",
    "min_sud_prosodic_features = find_prosodic_features(filename3)\n",
    "print(\"Prosodic Features for ED Audio: \\n\"+str(min_sud_prosodic_features)+\"\\n\")\n",
    "\n",
    "# For FLW-1 Audio\n",
    "medium_sud_prosodic_features = find_prosodic_features(filename2)\n",
    "print(\"Prosodic Features for FLW-1 Audio: \\n\"+str(medium_sud_prosodic_features)+\"\\n\")\n",
    "\n",
    "# For FLW-2 Audio\n",
    "high_sud_prosodic_features = find_prosodic_features(filename1)\n",
    "print(\"Prosodic Features for FLW-2 Audio: \\n\"+str(high_sud_prosodic_features)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for Prosodic Features:\n",
    "\n",
    "These are features that I got to know about from the Wiegersma paper, and are really helpful for understanding the emotional state of the patient.\n",
    "\n",
    "1. Phonation Rate: N(voiced minutes)/N(total minutes). The above observation shows that, for higher SUD values, the phonation rate drops, i.e, the patient is not able to speak much during higher SUDs.\n",
    "\n",
    "2. Speech Productivity: N(silent minutes)/N(voiced minutes). The above observation shows that silent segments increase as SUD value increases.\n",
    "\n",
    "3. Speech Rate: Words per minute. Speech rate decreased significantly for higher SUDs.\n",
    "\n",
    "4. Articulation Rate: Words per voiced minute. Articulation rate decreased significantly for higher SUDs.\n",
    "\n",
    "Indeed, these features give a good amount of insights into the emotional state of the patient!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
